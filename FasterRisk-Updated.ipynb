{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be99fbbb",
   "metadata": {},
   "source": [
    "# Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba8768b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:46:55.056885Z",
     "start_time": "2025-10-03T06:46:53.842043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STLMD   -> X_train (13324, 19), X_val (3282, 19), X_test (16425, 19)\n",
      "STLM    -> X_train (13324, 13), X_val (3282, 13), X_test (16425, 13)\n",
      "STPGLM  -> X_train (13324, 12), X_val (3282, 12), X_test (16425, 12)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "LABEL_COL = 'Cancer_lbl'\n",
    "ID_COL    = 'pid'\n",
    "\n",
    "FEATURES_STLMD  = ['sct_long_dia','part_solid','ground_glass','solid','Upper_Lobe','Spiculation','age','sex']\n",
    "FEATURES_STLM   = ['sct_long_dia','part_solid','ground_glass','solid','Upper_Lobe','Spiculation']\n",
    "FEATURES_STPGLM = ['sct_long_dia','part_solid','ground_glass','Upper_Lobe','Spiculation']\n",
    "\n",
    "# ≥ cutpoints for binning (age & size)\n",
    "CUTS_CONT = {\n",
    "    \"age\": [56, 58, 63, 68, 73],\n",
    "    \"sct_long_dia\": [4, 6, 8, 10, 12, 15, 20, 30],\n",
    "}\n",
    "\n",
    "# binary passthroughs (copied as-is)\n",
    "BIN_PASSTHROUGH_STLMD  = ['part_solid','ground_glass','solid','Upper_Lobe','Spiculation','sex']\n",
    "BIN_PASSTHROUGH_STLM   = ['part_solid','ground_glass','solid','Upper_Lobe','Spiculation']\n",
    "BIN_PASSTHROUGH_STPGLM = ['part_solid','ground_glass','Upper_Lobe','Spiculation']\n",
    "\n",
    "# Data files\n",
    "CSV1 = '/data/usr/ft42/CVIT_XAI/LungRADS_Modeling/NLST_Statistics/ml_dataset/nlst_ct_nodule_df_set1.csv'\n",
    "CSV2 = '/data/usr/ft42/CVIT_XAI/LungRADS_Modeling/NLST_Statistics/ml_dataset/nlst_ct_nodule_df_set2.csv'\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def _norm_text(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip().lower()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.replace('_','-').replace('–','-').replace('—','-')\n",
    "    return s\n",
    "\n",
    "def normalize_and_encode(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Canonicalize gender to {'male','female'}\n",
    "    - Canonicalize Nodule_Type to {'solid','ground-glass','part-solid'} using common aliases\n",
    "    - Create binaries: sex, part_solid, ground_glass, solid\n",
    "    - Coerce Upper_Lobe/Spiculation to ints if present\n",
    "    - Drop rows with unmapped gender/Nodule_Type; warn on non-exclusive types\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # gender\n",
    "    g = df['gender'].apply(_norm_text)\n",
    "    df['gender'] = g.map({'male':'male','m':'male','female':'female','f':'female'})\n",
    "\n",
    "    # nodule type\n",
    "    t = df['Nodule_Type'].apply(_norm_text)\n",
    "    mapping = {\n",
    "        'solid':'solid', 'sld':'solid',\n",
    "\n",
    "        'ground-glass':'ground-glass', 'ground glass':'ground-glass',\n",
    "        'ggo':'ground-glass', 'non-solid':'ground-glass',\n",
    "        'non solid':'ground-glass', 'nonsolid':'ground-glass',\n",
    "\n",
    "        'part-solid':'part-solid', 'part solid':'part-solid',\n",
    "        'semi-solid':'part-solid', 'semisolid':'part-solid', 'subsolid':'part-solid',\n",
    "    }\n",
    "    df['Nodule_Type'] = t.map(mapping)\n",
    "\n",
    "    # drop unmapped\n",
    "    bad = df['gender'].isna() | df['Nodule_Type'].isna()\n",
    "    if bad.any():\n",
    "        print(f\"[filter] Dropping {int(bad.sum())} rows with unmapped gender or Nodule_Type\")\n",
    "        # Uncomment to inspect:\n",
    "        # print(df.loc[bad, [ID_COL,'gender','Nodule_Type']].head(10))\n",
    "        df = df.loc[~bad].copy()\n",
    "\n",
    "    # binaries\n",
    "    df['sex']          = df['gender'].map({'male':0, 'female':1}).astype(int)\n",
    "    df['part_solid']   = (df['Nodule_Type'] == 'part-solid').astype(int)\n",
    "    df['ground_glass'] = (df['Nodule_Type'] == 'ground-glass').astype(int)\n",
    "    df['solid']        = (df['Nodule_Type'] == 'solid').astype(int)\n",
    "\n",
    "    # passthrough ints\n",
    "    for b in ['Upper_Lobe','Spiculation']:\n",
    "        if b in df.columns:\n",
    "            df[b] = pd.to_numeric(df[b], errors='ignore')\n",
    "            df[b] = pd.to_numeric(df[b], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # sanity: exactly one nodule type flag\n",
    "    onehot_sum = df[['solid','ground_glass','part_solid']].sum(axis=1)\n",
    "    if (onehot_sum != 1).any():\n",
    "        print(f\"[warn] {int((onehot_sum != 1).sum())} rows have non-exclusive nodule types; please inspect.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def to_fastrisk_y(y_raw, pos_label=1) -> np.ndarray:\n",
    "    \"\"\"Return 1-D np.ndarray[float] with labels in {-1.0, +1.0}.\"\"\"\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "    uniq = set(np.unique(y_arr))\n",
    "    if uniq <= {0, 1}:\n",
    "        return (2 * y_arr - 1).astype(float)\n",
    "    return np.where(y_arr == pos_label, 1.0, -1.0).astype(float)\n",
    "\n",
    "def make_ge_bins(df: pd.DataFrame, feature: str, cuts: list[float]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build columns like 'age >= 73' or 'sct_long_dia >= 20' (ASCII >=)\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    vals = pd.to_numeric(df[feature], errors='coerce')\n",
    "    for c in cuts:\n",
    "        col = f\"{feature} >= {c:g}\"\n",
    "        out[col] = (vals >= float(c)).astype(int)\n",
    "    return out\n",
    "\n",
    "def build_binary_matrix(X_df: pd.DataFrame,\n",
    "                        feature_cuts: dict[str, list[float]],\n",
    "                        passthrough_binary: list[str]) -> pd.DataFrame:\n",
    "    mats = []\n",
    "    for feat, cuts in feature_cuts.items():\n",
    "        if feat not in X_df.columns:\n",
    "            raise KeyError(f\"Missing feature for binning: {feat}\")\n",
    "        mats.append(make_ge_bins(X_df, feat, cuts))\n",
    "    for feat in passthrough_binary:\n",
    "        if feat not in X_df.columns:\n",
    "            raise KeyError(f\"Missing binary feature: {feat}\")\n",
    "        col = pd.Series(pd.to_numeric(X_df[feat], errors='coerce')).fillna(0).astype(int)\n",
    "        mats.append(pd.DataFrame({feat: col}, index=X_df.index))\n",
    "    return pd.concat(mats, axis=1)\n",
    "\n",
    "def binarize_and_align_custom(X_train_df: pd.DataFrame,\n",
    "                              X_val_df: pd.DataFrame,\n",
    "                              X_test_df: pd.DataFrame,\n",
    "                              feature_cuts: dict[str, list[float]],\n",
    "                              passthrough_binary: list[str]) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Binarize each split with ≥ cuts, then align val/test to training columns.\n",
    "    \"\"\"\n",
    "    X_train_bin = build_binary_matrix(X_train_df, feature_cuts, passthrough_binary)\n",
    "    X_val_bin   = build_binary_matrix(X_val_df,   feature_cuts, passthrough_binary)\n",
    "    X_test_bin  = build_binary_matrix(X_test_df,  feature_cuts, passthrough_binary)\n",
    "\n",
    "    def _align_like_train(train_bin_df: pd.DataFrame, other_bin_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        cols = list(train_bin_df.columns)\n",
    "        return other_bin_df.reindex(columns=cols, fill_value=0)\n",
    "\n",
    "    X_val_bin  = _align_like_train(X_train_bin, X_val_bin)\n",
    "    X_test_bin = _align_like_train(X_train_bin, X_test_bin)\n",
    "\n",
    "    # sanity\n",
    "    assert list(X_val_bin.columns)  == list(X_train_bin.columns)\n",
    "    assert list(X_test_bin.columns) == list(X_train_bin.columns)\n",
    "    return X_train_bin, X_val_bin, X_test_bin\n",
    "\n",
    "def prepare_data(df: pd.DataFrame, feature_cols: list[str], label_col: str):\n",
    "    X = df[feature_cols]\n",
    "    y = df[label_col]\n",
    "    return X, y\n",
    "\n",
    "# -------------------------\n",
    "# Load, normalize/encode\n",
    "# -------------------------\n",
    "df1 = pd.read_csv(CSV1)\n",
    "df2 = pd.read_csv(CSV2)\n",
    "\n",
    "df1 = normalize_and_encode(df1)\n",
    "df2 = normalize_and_encode(df2)\n",
    "\n",
    "# -------------------------\n",
    "# Patient-level stratified split (on df1)\n",
    "# -------------------------\n",
    "patients = df1[[ID_COL, LABEL_COL]].drop_duplicates()\n",
    "train_patients, val_patients = train_test_split(\n",
    "    patients,\n",
    "    test_size=0.2,\n",
    "    stratify=patients[LABEL_COL],\n",
    "    random_state=42\n",
    ")\n",
    "train_df = df1[df1[ID_COL].isin(train_patients[ID_COL])]\n",
    "val_df   = df1[df1[ID_COL].isin(val_patients[ID_COL])]\n",
    "\n",
    "# ============================================================\n",
    "# STLMD (includes age & sex)  —  bins: age >= c, sct_long_dia >= c\n",
    "# ============================================================\n",
    "X_train_STLMD_df, y_train_STLMD_raw = prepare_data(train_df, FEATURES_STLMD, LABEL_COL)\n",
    "X_val_STLMD_df,   y_val_STLMD_raw   = prepare_data(val_df,   FEATURES_STLMD, LABEL_COL)\n",
    "X_test_STLMD_df,  y_test_STLMD_raw  = prepare_data(df2,      FEATURES_STLMD, LABEL_COL)\n",
    "\n",
    "X_train_STLMD_bin, X_val_STLMD_bin, X_test_STLMD_bin = binarize_and_align_custom(\n",
    "    X_train_STLMD_df, X_val_STLMD_df, X_test_STLMD_df,\n",
    "    feature_cuts={\"age\": CUTS_CONT[\"age\"], \"sct_long_dia\": CUTS_CONT[\"sct_long_dia\"]},\n",
    "    passthrough_binary=BIN_PASSTHROUGH_STLMD\n",
    ")\n",
    "\n",
    "y_train_STLMD = to_fastrisk_y(y_train_STLMD_raw, pos_label=1)\n",
    "y_val_STLMD   = to_fastrisk_y(y_val_STLMD_raw,   pos_label=1)\n",
    "y_test_STLMD  = to_fastrisk_y(y_test_STLMD_raw,  pos_label=1)\n",
    "\n",
    "X_train_STLMD = X_train_STLMD_bin.to_numpy(dtype=float)\n",
    "X_val_STLMD   = X_val_STLMD_bin.to_numpy(dtype=float)\n",
    "X_test_STLMD  = X_test_STLMD_bin.to_numpy(dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# STLM (no age/sex) — bins: sct_long_dia >= c\n",
    "# ============================================================\n",
    "X_train_STLM_df, y_train_STLM_raw = prepare_data(train_df, FEATURES_STLM, LABEL_COL)\n",
    "X_val_STLM_df,   y_val_STLM_raw   = prepare_data(val_df,   FEATURES_STLM, LABEL_COL)\n",
    "X_test_STLM_df,  y_test_STLM_raw  = prepare_data(df2,      FEATURES_STLM, LABEL_COL)\n",
    "\n",
    "X_train_STLM_bin, X_val_STLM_bin, X_test_STLM_bin = binarize_and_align_custom(\n",
    "    X_train_STLM_df, X_val_STLM_df, X_test_STLM_df,\n",
    "    feature_cuts={\"sct_long_dia\": CUTS_CONT[\"sct_long_dia\"]},\n",
    "    passthrough_binary=BIN_PASSTHROUGH_STLM\n",
    ")\n",
    "\n",
    "y_train_STLM = to_fastrisk_y(y_train_STLM_raw, pos_label=1)\n",
    "y_val_STLM   = to_fastrisk_y(y_val_STLM_raw,   pos_label=1)\n",
    "y_test_STLM  = to_fastrisk_y(y_test_STLM_raw,  pos_label=1)\n",
    "\n",
    "X_train_STLM = X_train_STLM_bin.to_numpy(dtype=float)\n",
    "X_val_STLM   = X_val_STLM_bin.to_numpy(dtype=float)\n",
    "X_test_STLM  = X_test_STLM_bin.to_numpy(dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# STPGLM (subset features) — bins: sct_long_dia >= c\n",
    "# ============================================================\n",
    "X_train_STPGLM_df, y_train_STPGLM_raw = prepare_data(train_df, FEATURES_STPGLM, LABEL_COL)\n",
    "X_val_STPGLM_df,   y_val_STPGLM_raw   = prepare_data(val_df,   FEATURES_STPGLM, LABEL_COL)\n",
    "X_test_STPGLM_df,  y_test_STPGLM_raw  = prepare_data(df2,      FEATURES_STPGLM, LABEL_COL)\n",
    "\n",
    "X_train_STPGLM_bin, X_val_STPGLM_bin, X_test_STPGLM_bin = binarize_and_align_custom(\n",
    "    X_train_STPGLM_df, X_val_STPGLM_df, X_test_STPGLM_df,\n",
    "    feature_cuts={\"sct_long_dia\": CUTS_CONT[\"sct_long_dia\"]},\n",
    "    passthrough_binary=BIN_PASSTHROUGH_STPGLM\n",
    ")\n",
    "\n",
    "y_train_STPGLM = to_fastrisk_y(y_train_STPGLM_raw, pos_label=1)\n",
    "y_val_STPGLM   = to_fastrisk_y(y_val_STPGLM_raw,   pos_label=1)\n",
    "y_test_STPGLM  = to_fastrisk_y(y_test_STPGLM_raw,  pos_label=1)\n",
    "\n",
    "X_train_STPGLM = X_train_STPGLM_bin.to_numpy(dtype=float)\n",
    "X_val_STPGLM   = X_val_STPGLM_bin.to_numpy(dtype=float)\n",
    "X_test_STPGLM  = X_test_STPGLM_bin.to_numpy(dtype=float)\n",
    "\n",
    "# -------------------------\n",
    "# Quick hygiene checks\n",
    "# -------------------------\n",
    "def _chk(Xtr, ytr, Xv, yv, Xte, yte, name):\n",
    "    assert Xtr.shape[0] == ytr.shape[0] and Xv.shape[0] == yv.shape[0] and Xte.shape[0] == yte.shape[0], f\"row mismatch in {name}\"\n",
    "    assert set(np.unique(ytr)) <= {-1.0, 1.0} and set(np.unique(yv)) <= {-1.0, 1.0} and set(np.unique(yte)) <= {-1.0, 1.0}, f\"bad labels in {name}\"\n",
    "    print(f\"{name:7s} -> X_train {Xtr.shape}, X_val {Xv.shape}, X_test {Xte.shape}\")\n",
    "\n",
    "_chk(X_train_STLMD, y_train_STLMD, X_val_STLMD, y_val_STLMD, X_test_STLMD, y_test_STLMD, \"STLMD\")\n",
    "_chk(X_train_STLM,  y_train_STLM,  X_val_STLM,  y_val_STLM,  X_test_STLM,  y_test_STLM,  \"STLM\")\n",
    "_chk(X_train_STPGLM,y_train_STPGLM,X_val_STPGLM,y_val_STPGLM,X_test_STPGLM,y_test_STPGLM,\"STPGLM\")\n",
    "\n",
    "# Optional: feature names for each design\n",
    "FEATURE_NAMES_STLMD  = list(X_train_STLMD_bin.columns)\n",
    "FEATURE_NAMES_STLM   = list(X_train_STLM_bin.columns)\n",
    "FEATURE_NAMES_STPGLM = list(X_train_STPGLM_bin.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b09f6cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T06:46:55.065217Z",
     "start_time": "2025-10-03T06:46:55.058671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age >= 56',\n",
       " 'age >= 58',\n",
       " 'age >= 63',\n",
       " 'age >= 68',\n",
       " 'age >= 73',\n",
       " 'sct_long_dia >= 4',\n",
       " 'sct_long_dia >= 6',\n",
       " 'sct_long_dia >= 8',\n",
       " 'sct_long_dia >= 10',\n",
       " 'sct_long_dia >= 12',\n",
       " 'sct_long_dia >= 15',\n",
       " 'sct_long_dia >= 20',\n",
       " 'sct_long_dia >= 30',\n",
       " 'part_solid',\n",
       " 'ground_glass',\n",
       " 'solid',\n",
       " 'Upper_Lobe',\n",
       " 'Spiculation',\n",
       " 'sex']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURE_NAMES_STLMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dadfc0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-03T07:01:32.464206Z",
     "start_time": "2025-10-03T06:58:58.422704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing: STLMD ===\n",
      "[opt] gap_tolerance=0.15, select_top_m=100, maxAttempts=200, k=5, parent_size=50\n",
      "Optimization takes 135.32 seconds.\n",
      "We generate 71 risk score models from the sparse diverse pool\n",
      "\n",
      "=== Per-model summaries ===\n",
      "[Card 01] b0=-7.000, mult=2.430, #terms=5\n",
      "  Train AUC: 0.705 | Test AUC: 0.697\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), solid(-1), Spiculation(+2)\n",
      "[Card 02] b0=-12.000, mult=3.272, #terms=5\n",
      "  Train AUC: 0.707 | Test AUC: 0.714\n",
      "  Features : age >= 58(+2), age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), Spiculation(+2)\n",
      "[Card 03] b0=-11.000, mult=3.519, #terms=5\n",
      "  Train AUC: 0.708 | Test AUC: 0.717\n",
      "  Features : age >= 58(+2), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), solid(-2), Spiculation(+2)\n",
      "[Card 04] b0=-11.000, mult=3.977, #terms=5\n",
      "  Train AUC: 0.704 | Test AUC: 0.697\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+4), sct_long_dia >= 15(+3), solid(-2), Spiculation(+3)\n",
      "[Card 05] b0=-10.000, mult=2.980, #terms=5\n",
      "  Train AUC: 0.707 | Test AUC: 0.694\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+2), Upper_Lobe(+1), Spiculation(+2)\n",
      "[Card 06] b0=-10.000, mult=3.518, #terms=5\n",
      "  Train AUC: 0.711 | Test AUC: 0.725\n",
      "  Features : age >= 68(+2), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), solid(-2), Spiculation(+2)\n",
      "[Card 07] b0=-7.000, mult=2.242, #terms=5\n",
      "  Train AUC: 0.712 | Test AUC: 0.706\n",
      "  Features : age >= 58(+1), age >= 73(+2), sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), solid(-1)\n",
      "[Card 08] b0=-13.000, mult=4.235, #terms=5\n",
      "  Train AUC: 0.710 | Test AUC: 0.686\n",
      "  Features : age >= 73(+5), sct_long_dia >= 8(+4), sct_long_dia >= 12(+4), solid(-2), Upper_Lobe(+2)\n",
      "[Card 09] b0=-11.000, mult=3.280, #terms=5\n",
      "  Train AUC: 0.702 | Test AUC: 0.691\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), Spiculation(+2), sex(+1)\n",
      "[Card 10] b0=-11.000, mult=3.346, #terms=5\n",
      "  Train AUC: 0.696 | Test AUC: 0.690\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), part_solid(+2), Spiculation(+2)\n",
      "[Card 11] b0=-10.000, mult=3.644, #terms=5\n",
      "  Train AUC: 0.704 | Test AUC: 0.698\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+3), sct_long_dia >= 10(+2), solid(-2), Spiculation(+3)\n",
      "[Card 12] b0=-10.000, mult=3.480, #terms=5\n",
      "  Train AUC: 0.707 | Test AUC: 0.700\n",
      "  Features : age >= 73(+4), sct_long_dia >= 6(+2), sct_long_dia >= 12(+4), solid(-2), Spiculation(+3)\n",
      "[Card 13] b0=-9.000, mult=2.655, #terms=5\n",
      "  Train AUC: 0.706 | Test AUC: 0.710\n",
      "  Features : age >= 73(+3), sct_long_dia >= 6(+1), sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), Spiculation(+2)\n",
      "[Card 14] b0=-8.000, mult=2.454, #terms=5\n",
      "  Train AUC: 0.700 | Test AUC: 0.697\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), ground_glass(+1), Spiculation(+2)\n",
      "[Card 15] b0=-11.000, mult=3.377, #terms=5\n",
      "  Train AUC: 0.712 | Test AUC: 0.715\n",
      "  Features : age >= 58(+2), age >= 73(+3), sct_long_dia >= 8(+4), solid(-2), Spiculation(+3)\n",
      "[Card 16] b0=-11.000, mult=3.380, #terms=5\n",
      "  Train AUC: 0.692 | Test AUC: 0.690\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), sct_long_dia >= 30(+3), Spiculation(+2)\n",
      "[Card 17] b0=-12.000, mult=3.100, #terms=5\n",
      "  Train AUC: 0.696 | Test AUC: 0.697\n",
      "  Features : age >= 56(+2), age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), Spiculation(+2)\n",
      "[Card 18] b0=-10.000, mult=3.523, #terms=5\n",
      "  Train AUC: 0.707 | Test AUC: 0.724\n",
      "  Features : age >= 63(+1), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), solid(-2), Spiculation(+2)\n",
      "[Card 19] b0=-11.000, mult=3.323, #terms=5\n",
      "  Train AUC: 0.704 | Test AUC: 0.725\n",
      "  Features : age >= 63(+1), age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), Spiculation(+2)\n",
      "[Card 20] b0=-10.000, mult=3.459, #terms=5\n",
      "  Train AUC: 0.703 | Test AUC: 0.698\n",
      "  Features : sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), solid(-2), Upper_Lobe(+1), Spiculation(+2)\n",
      "[Card 21] b0=-10.000, mult=3.059, #terms=5\n",
      "  Train AUC: 0.704 | Test AUC: 0.720\n",
      "  Features : age >= 68(+1), age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+2), Spiculation(+2)\n",
      "[Card 22] b0=-12.000, mult=3.748, #terms=5\n",
      "  Train AUC: 0.697 | Test AUC: 0.704\n",
      "  Features : age >= 56(+2), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), solid(-2), Spiculation(+3)\n",
      "[Card 23] b0=-7.000, mult=2.666, #terms=5\n",
      "  Train AUC: 0.702 | Test AUC: 0.693\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 20(+2), solid(-2), Spiculation(+2)\n",
      "[Card 24] b0=-9.000, mult=3.019, #terms=5\n",
      "  Train AUC: 0.711 | Test AUC: 0.703\n",
      "  Features : age >= 73(+3), sct_long_dia >= 6(+1), sct_long_dia >= 8(+2), sct_long_dia >= 12(+3), solid(-1)\n",
      "[Card 25] b0=-9.000, mult=3.290, #terms=5\n",
      "  Train AUC: 0.702 | Test AUC: 0.694\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+4), sct_long_dia >= 30(+4), solid(-2), Spiculation(+3)\n",
      "[Card 26] b0=-11.000, mult=3.411, #terms=5\n",
      "  Train AUC: 0.692 | Test AUC: 0.691\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), sct_long_dia >= 20(+1), Spiculation(+2)\n",
      "[Card 27] b0=-10.000, mult=3.486, #terms=5\n",
      "  Train AUC: 0.709 | Test AUC: 0.696\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+4), solid(-2), Upper_Lobe(+1), Spiculation(+3)\n",
      "[Card 28] b0=-9.000, mult=2.945, #terms=5\n",
      "  Train AUC: 0.704 | Test AUC: 0.681\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), solid(-1), sex(+1)\n",
      "[Card 29] b0=-7.000, mult=2.149, #terms=5\n",
      "  Train AUC: 0.703 | Test AUC: 0.690\n",
      "  Features : age >= 56(+1), age >= 73(+2), sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), solid(-1)\n",
      "[Card 30] b0=-9.000, mult=2.818, #terms=5\n",
      "  Train AUC: 0.692 | Test AUC: 0.692\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+2), sct_long_dia >= 10(+1), sct_long_dia >= 12(+2), Spiculation(+2)\n",
      "[Card 31] b0=-8.000, mult=2.676, #terms=5\n",
      "  Train AUC: 0.703 | Test AUC: 0.713\n",
      "  Features : sct_long_dia >= 6(+1), sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), solid(-1), Spiculation(+2)\n",
      "[Card 32] b0=-9.000, mult=3.030, #terms=5\n",
      "  Train AUC: 0.709 | Test AUC: 0.714\n",
      "  Features : age >= 68(+1), age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), solid(-1)\n",
      "[Card 33] b0=-10.000, mult=3.044, #terms=5\n",
      "  Train AUC: 0.691 | Test AUC: 0.691\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+2), sct_long_dia >= 15(+1), Spiculation(+2)\n",
      "[Card 34] b0=-8.000, mult=2.833, #terms=5\n",
      "  Train AUC: 0.700 | Test AUC: 0.685\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+2), sct_long_dia >= 30(+2), solid(-1)\n",
      "[Card 35] b0=-11.000, mult=3.963, #terms=5\n",
      "  Train AUC: 0.698 | Test AUC: 0.690\n",
      "  Features : sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), solid(-2), Spiculation(+3), sex(+1)\n",
      "[Card 36] b0=-11.000, mult=3.786, #terms=5\n",
      "  Train AUC: 0.709 | Test AUC: 0.713\n",
      "  Features : age >= 63(+1), age >= 73(+4), sct_long_dia >= 8(+4), sct_long_dia >= 12(+3), solid(-2)\n",
      "[Card 37] b0=-7.000, mult=2.485, #terms=5\n",
      "  Train AUC: 0.694 | Test AUC: 0.698\n",
      "  Features : sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), sct_long_dia >= 30(+2), solid(-1), Spiculation(+2)\n",
      "[Card 38] b0=-11.000, mult=3.973, #terms=5\n",
      "  Train AUC: 0.700 | Test AUC: 0.686\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+4), sct_long_dia >= 12(+3), sct_long_dia >= 20(+2), solid(-2)\n",
      "[Card 39] b0=-11.000, mult=3.356, #terms=5\n",
      "  Train AUC: 0.705 | Test AUC: 0.699\n",
      "  Features : age >= 56(+2), age >= 73(+4), sct_long_dia >= 8(+4), solid(-2), Spiculation(+3)\n",
      "[Card 40] b0=-10.000, mult=3.491, #terms=5\n",
      "  Train AUC: 0.705 | Test AUC: 0.686\n",
      "  Features : age >= 73(+4), sct_long_dia >= 8(+4), solid(-2), Spiculation(+3), sex(+1)\n",
      "[Card 41] b0=-9.000, mult=3.200, #terms=5\n",
      "  Train AUC: 0.710 | Test AUC: 0.717\n",
      "  Features : age >= 68(+1), age >= 73(+3), sct_long_dia >= 8(+4), solid(-2), Spiculation(+3)\n",
      "[Card 42] b0=-7.000, mult=2.491, #terms=5\n",
      "  Train AUC: 0.700 | Test AUC: 0.684\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), sct_long_dia >= 15(+1), solid(-1)\n",
      "[Card 43] b0=-9.000, mult=3.415, #terms=5\n",
      "  Train AUC: 0.689 | Test AUC: 0.681\n",
      "  Features : age >= 73(+4), sct_long_dia >= 10(+3), sct_long_dia >= 12(+2), solid(-2), Spiculation(+3)\n",
      "[Card 44] b0=-8.000, mult=3.007, #terms=5\n",
      "  Train AUC: 0.700 | Test AUC: 0.685\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), ground_glass(-1), solid(-2)\n",
      "[Card 45] b0=-9.000, mult=3.007, #terms=5\n",
      "  Train AUC: 0.700 | Test AUC: 0.685\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), sct_long_dia >= 12(+3), part_solid(+1), solid(-1)\n",
      "[Card 46] b0=-9.000, mult=3.188, #terms=5\n",
      "  Train AUC: 0.710 | Test AUC: 0.706\n",
      "  Features : age >= 73(+4), sct_long_dia >= 6(+1), sct_long_dia >= 8(+3), solid(-2), Spiculation(+3)\n",
      "[Card 47] b0=-10.000, mult=3.723, #terms=5\n",
      "  Train AUC: 0.694 | Test AUC: 0.698\n",
      "  Features : sct_long_dia >= 8(+3), sct_long_dia >= 12(+2), sct_long_dia >= 15(+1), solid(-2), Spiculation(+3)\n",
      "[Card 48] b0=-7.000, mult=2.484, #terms=5\n",
      "  Train AUC: 0.694 | Test AUC: 0.696\n",
      "  Features : sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), part_solid(+1), solid(-1), Spiculation(+2)\n",
      "[Card 49] b0=-6.000, mult=2.484, #terms=5\n",
      "  Train AUC: 0.694 | Test AUC: 0.696\n",
      "  Features : sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), ground_glass(-1), solid(-2), Spiculation(+2)\n",
      "[Card 50] b0=-7.000, mult=2.485, #terms=5\n",
      "  Train AUC: 0.694 | Test AUC: 0.698\n",
      "  Features : sct_long_dia >= 8(+2), sct_long_dia >= 12(+2), sct_long_dia >= 20(+1), solid(-1), Spiculation(+2)\n",
      "[Card 51] b0=-7.000, mult=2.453, #terms=5\n",
      "  Train AUC: 0.699 | Test AUC: 0.686\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+2), sct_long_dia >= 10(+1), sct_long_dia >= 12(+2), solid(-1)\n",
      "[Card 52] b0=-10.000, mult=3.538, #terms=5\n",
      "  Train AUC: 0.710 | Test AUC: 0.721\n",
      "  Features : age >= 63(+1), age >= 73(+3), sct_long_dia >= 8(+4), solid(-2), Spiculation(+3)\n",
      "[Card 53] b0=-10.000, mult=3.752, #terms=5\n",
      "  Train AUC: 0.694 | Test AUC: 0.699\n",
      "  Features : sct_long_dia >= 8(+3), sct_long_dia >= 10(+1), sct_long_dia >= 12(+2), solid(-2), Spiculation(+3)\n",
      "[Card 54] b0=-8.000, mult=1.290, #terms=5\n",
      "  Train AUC: 0.690 | Test AUC: 0.691\n",
      "  Features : age >= 73(+1), sct_long_dia >= 4(+4), sct_long_dia >= 8(+1), sct_long_dia >= 12(+1), Spiculation(+1)\n",
      "[Card 55] b0=-9.000, mult=2.936, #terms=5\n",
      "  Train AUC: 0.694 | Test AUC: 0.692\n",
      "  Features : age >= 58(+2), age >= 73(+3), sct_long_dia >= 12(+4), solid(-2), Spiculation(+3)\n",
      "[Card 56] b0=-7.000, mult=2.343, #terms=5\n",
      "  Train AUC: 0.702 | Test AUC: 0.692\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), part_solid(+1), solid(-1), Spiculation(+2)\n",
      "[Card 57] b0=-6.000, mult=2.343, #terms=5\n",
      "  Train AUC: 0.702 | Test AUC: 0.692\n",
      "  Features : age >= 73(+3), sct_long_dia >= 8(+3), ground_glass(-1), solid(-2), Spiculation(+2)\n",
      "[Card 58] b0=-7.000, mult=1.207, #terms=5\n",
      "  Train AUC: 0.692 | Test AUC: 0.691\n",
      "  Features : sct_long_dia >= 4(+4), sct_long_dia >= 8(+1), sct_long_dia >= 12(+1), solid(-1), Spiculation(+1)\n",
      "[Card 59] b0=-8.000, mult=3.024, #terms=5\n",
      "  Train AUC: 0.693 | Test AUC: 0.667\n",
      "  Features : age >= 73(+4), sct_long_dia >= 12(+4), solid(-2), Upper_Lobe(+1), Spiculation(+3)\n",
      "[Card 60] b0=-8.000, mult=1.460, #terms=5\n",
      "  Train AUC: 0.700 | Test AUC: 0.689\n",
      "  Features : age >= 73(+2), sct_long_dia >= 4(+4), sct_long_dia >= 8(+2), solid(-1), Spiculation(+1)\n",
      "[Card 61] b0=-9.000, mult=2.913, #terms=5\n",
      "  Train AUC: 0.686 | Test AUC: 0.672\n",
      "  Features : age >= 56(+2), age >= 73(+3), sct_long_dia >= 12(+4), solid(-2), Spiculation(+3)\n",
      "[Card 62] b0=-8.000, mult=3.006, #terms=5\n",
      "  Train AUC: 0.690 | Test AUC: 0.653\n",
      "  Features : age >= 73(+4), sct_long_dia >= 12(+4), solid(-2), Spiculation(+3), sex(+1)\n",
      "[Card 63] b0=-8.000, mult=3.063, #terms=5\n",
      "  Train AUC: 0.693 | Test AUC: 0.700\n",
      "  Features : age >= 63(+1), age >= 73(+3), sct_long_dia >= 12(+4), solid(-2), Spiculation(+3)\n",
      "[Card 64] b0=-7.000, mult=2.792, #terms=5\n",
      "  Train AUC: 0.694 | Test AUC: 0.693\n",
      "  Features : age >= 68(+1), age >= 73(+2), sct_long_dia >= 12(+4), solid(-2), Spiculation(+2)\n",
      "[Card 65] b0=-7.000, mult=2.904, #terms=5\n",
      "  Train AUC: 0.682 | Test AUC: 0.659\n",
      "  Features : age >= 73(+3), sct_long_dia >= 12(+4), sct_long_dia >= 30(+2), solid(-2), Spiculation(+2)\n",
      "[Card 66] b0=-7.000, mult=1.243, #terms=5\n",
      "  Train AUC: 0.695 | Test AUC: 0.677\n",
      "  Features : age >= 73(+1), sct_long_dia >= 4(+4), sct_long_dia >= 8(+1), sct_long_dia >= 12(+1), solid(-1)\n",
      "[Card 67] b0=-8.000, mult=3.103, #terms=5\n",
      "  Train AUC: 0.682 | Test AUC: 0.662\n",
      "  Features : age >= 73(+4), sct_long_dia >= 12(+4), sct_long_dia >= 20(+1), solid(-2), Spiculation(+3)\n",
      "[Card 68] b0=-8.000, mult=3.242, #terms=5\n",
      "  Train AUC: 0.682 | Test AUC: 0.662\n",
      "  Features : age >= 73(+4), sct_long_dia >= 12(+4), sct_long_dia >= 15(+1), solid(-2), Spiculation(+3)\n",
      "[Card 69] b0=-6.000, mult=2.232, #terms=5\n",
      "  Train AUC: 0.682 | Test AUC: 0.661\n",
      "  Features : age >= 73(+3), sct_long_dia >= 12(+3), part_solid(+1), solid(-1), Spiculation(+2)\n",
      "[Card 70] b0=-5.000, mult=2.232, #terms=5\n",
      "  Train AUC: 0.682 | Test AUC: 0.661\n",
      "  Features : age >= 73(+3), sct_long_dia >= 12(+3), ground_glass(-1), solid(-2), Spiculation(+2)\n",
      "[Card 71] b0=-9.000, mult=1.564, #terms=5\n",
      "  Train AUC: 0.681 | Test AUC: 0.661\n",
      "  Features : age >= 73(+2), sct_long_dia >= 4(+5), sct_long_dia >= 12(+2), solid(-1), Spiculation(+2)\n",
      "[AUC] Wrote ./FasterRisk_STLMD_Outputs/model_metrics_STLMD.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ROC] Saved subplot page: ./FasterRisk_STLMD_Outputs/roc_grid_STLMD_train_p1.png\n",
      "[ROC] Saved subplot page: ./FasterRisk_STLMD_Outputs/roc_grid_STLMD_train_p2.png\n",
      "[ROC] Saved subplot page: ./FasterRisk_STLMD_Outputs/roc_grid_STLMD_test_p1.png\n",
      "[ROC] Saved subplot page: ./FasterRisk_STLMD_Outputs/roc_grid_STLMD_test_p2.png\n",
      "[Riskomon] Wrote ./FasterRisk_STLMD_Outputs/CANCER_STLMD.json with 71 models (memo schema)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Training + Evaluation (with Riskomon export)\n",
    "# ============================\n",
    "import os, time, json, math\n",
    "from typing import List, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "OUTDIR = \"./FasterRisk_STLMD_Outputs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---- knobs you can tune ----\n",
    "SPARSITY_K        = 5\n",
    "PARENT_SIZE       = 50\n",
    "\n",
    "# Rashomon / diversity knobs (documented in FasterRisk)\n",
    "GAP_TOLERANCE     = 0.15   # larger -> accept more near-optimal models\n",
    "SELECT_TOP_M      = 100    # keep more diverse supports\n",
    "MAX_ATTEMPTS      = 200    # try more swaps/diversification\n",
    "\n",
    "# choose which designs to run\n",
    "DESIGNS_TO_RUN = [\"STLMD\"]   # e.g. [\"STLMD\",\"STLM\",\"STPGLM\"]\n",
    "\n",
    "# ROC subplot grid (50 per page)\n",
    "SUBPLOT_ROWS = 10\n",
    "SUBPLOT_COLS = 5\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Version-robust optimizer creation\n",
    "# ---------------------------------------------\n",
    "def make_optimizer(X, y,\n",
    "                   k=SPARSITY_K, parent_size=PARENT_SIZE,\n",
    "                   gap_tolerance=GAP_TOLERANCE,\n",
    "                   select_top_m=SELECT_TOP_M,\n",
    "                   max_attempts=MAX_ATTEMPTS,\n",
    "                   want_intercept=True):\n",
    "    \n",
    "    # (1) gap + select_top_m + maxAttempts (camelCase)\n",
    "   \n",
    "        return RiskScoreOptimizer(\n",
    "            X=X, y=y, k=k, parent_size=parent_size,\n",
    "            gap_tolerance=gap_tolerance,\n",
    "            select_top_m=select_top_m,\n",
    "            maxAttempts=max_attempts)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# get_models() return format helper (2- or 3-tuple)\n",
    "# ---------------------------------------------\n",
    "def extract_models(ret):\n",
    "    if isinstance(ret, tuple) and len(ret) == 2:\n",
    "        multipliers, integer_mat = ret\n",
    "        integer_mat = np.asarray(integer_mat, dtype=int)\n",
    "        beta0_int = integer_mat[:, 0]\n",
    "        betas_int = integer_mat[:, 1:]\n",
    "        return np.asarray(multipliers, float), beta0_int, betas_int\n",
    "    elif isinstance(ret, tuple) and len(ret) == 3:\n",
    "        multipliers, beta0_int, betas_int = ret\n",
    "        return (np.asarray(multipliers, float),\n",
    "                np.asarray(beta0_int, int),\n",
    "                np.asarray(betas_int, int))\n",
    "    else:\n",
    "        raise RuntimeError(\"Unexpected return format from get_models()\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Metrics + ROC plotting\n",
    "# ---------------------------------------------\n",
    "def model_probs(mult: float, b0: float, betas: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "    z = b0 + X @ betas.astype(float)\n",
    "    return 1.0 / (1.0 + np.exp(-mult * z))\n",
    "\n",
    "def compute_model_metrics(multipliers, intercepts, coef_matrix, X, y):\n",
    "    aucs, accs, n_terms, probs_list = [], [], [], []\n",
    "    y01 = ((y + 1.0) / 2.0)  # {-1,+1} -> {0,1}\n",
    "    for m, b0, w in zip(multipliers, intercepts, coef_matrix):\n",
    "        w = np.asarray(w, dtype=int)\n",
    "        p = model_probs(float(m), float(b0), w, X)\n",
    "        yhat = (p >= 0.5).astype(int)\n",
    "        acc = (yhat == y01).mean()\n",
    "        fpr, tpr, _ = roc_curve(y01, p)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "        aucs.append(float(auc_val))\n",
    "        accs.append(float(acc))\n",
    "        n_terms.append(int((w != 0).sum()))\n",
    "        probs_list.append(p)\n",
    "    return np.asarray(aucs), np.asarray(accs), np.asarray(n_terms), probs_list\n",
    "\n",
    "def print_model_summaries(multipliers, intercepts, coef_matrix, feature_names, train_aucs, test_aucs):\n",
    "    width = max(2, len(str(len(multipliers))))\n",
    "    print(\"\\n=== Per-model summaries ===\")\n",
    "    for i, (m, b0, w) in enumerate(zip(multipliers, intercepts, coef_matrix)):\n",
    "        card = f\"{i+1:0{width}d}\"\n",
    "        w = np.asarray(w, dtype=int)\n",
    "        nz = np.flatnonzero(w)\n",
    "        feats = [f\"{feature_names[j]}({int(w[j]):+d})\" for j in nz]\n",
    "        feats_str = \", \".join(feats) if feats else \"(no terms)\"\n",
    "        print(f\"[Card {card}] b0={float(b0):+.3f}, mult={float(m):.3f}, #terms={len(nz)}\")\n",
    "        print(f\"  Train AUC: {train_aucs[i]:.3f} | Test AUC: {test_aucs[i]:.3f}\")\n",
    "        print(f\"  Features : {feats_str}\")\n",
    "\n",
    "def plot_roc_subplots_all(y_true01, probs_list, labels, rows, cols, title_prefix, out_prefix):\n",
    "    from math import ceil\n",
    "    per_page = rows * cols\n",
    "    total = len(probs_list)\n",
    "    pages = max(1, ceil(total / per_page))\n",
    "    for pg in range(pages):\n",
    "        s, e = pg * per_page, min((pg + 1) * per_page, total)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2), squeeze=False)\n",
    "        k = 0\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                ax = axes[r, c]\n",
    "                if s + k < e:\n",
    "                    p = probs_list[s + k]\n",
    "                    lab = labels[s + k]\n",
    "                    fpr, tpr, _ = roc_curve(y_true01, p)\n",
    "                    auc_val = auc(fpr, tpr)\n",
    "                    ax.plot(fpr, tpr, color='purple',label=f\"(AUC={auc_val:.2f})\")\n",
    "                    ax.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "                    ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "                    ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\")\n",
    "                    ax.legend(loc=\"lower right\", fontsize=8)\n",
    "                    ax.set_title(lab, fontsize=10)\n",
    "                    k += 1\n",
    "                else:\n",
    "                    ax.axis(\"off\")\n",
    "        fig.suptitle(f\"{title_prefix}  (models {s+1}-{e})\", fontsize=14)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        out_png = os.path.join(OUTDIR, f\"{out_prefix}_p{pg+1}.png\")\n",
    "        fig.savefig(out_png, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"[ROC] Saved subplot page: {out_png}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Riskomon (memo) exporter helpers\n",
    "# ---------------------------------------------\n",
    "def _sigmoid(z: float) -> float:\n",
    "    return 1.0 / (1.0 + math.exp(-z))\n",
    "\n",
    "def _score_span(coefs: np.ndarray) -> Tuple[int, int]:\n",
    "    \"\"\"Min/max integer score contribution from sparse integer weights (no intercept).\"\"\"\n",
    "    pos = int(coefs[coefs > 0].sum()) if coefs.size else 0\n",
    "    neg = int(coefs[coefs < 0].sum()) if coefs.size else 0\n",
    "    return neg, pos\n",
    "\n",
    "def _risk_scale(multiplier: float, intercept: float, coefs: np.ndarray) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Returns [[score, prob], ...] with probability = sigmoid(multiplier * (intercept + score)).\n",
    "    (Score here is the sum of integer feature weights; intercept is separate.)\n",
    "    \"\"\"\n",
    "    smin, smax = _score_span(coefs)\n",
    "    out = []\n",
    "    for s in range(smin, smax + 1):\n",
    "        total = intercept + s\n",
    "        p = _sigmoid(multiplier * total)\n",
    "        out.append([float(s), float(p)])\n",
    "    return out\n",
    "\n",
    "def _feature_pairs(coefs: np.ndarray, feat_names: List[str]) -> List[List[Any]]:\n",
    "    \"\"\"\n",
    "    [[coef, \"FeatureName\"], ...] — coef first, only nonzeros, sorted by |coef| desc.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for w, name in zip(coefs.tolist(), feat_names):\n",
    "        if int(w) != 0:\n",
    "            pairs.append([float(int(w)), str(name)])\n",
    "    pairs.sort(key=lambda x: (-abs(x[0]), x[1]))\n",
    "    return pairs\n",
    "\n",
    "def export_riskomon_payload_memo(\n",
    "    multipliers: List[float],\n",
    "    intercepts: List[int],\n",
    "    coef_matrix: List[np.ndarray],\n",
    "    feature_names: List[str],\n",
    "    X_train: np.ndarray, y_train: np.ndarray,\n",
    "    dataset_tag: str = \"CANCER_STLMD\",\n",
    "    export_n: int | None = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build JSON using memo schema:\n",
    "      - feature_data: [[coef, \"name\"], ...]\n",
    "      - risk_scale:   [[score, prob], ...]\n",
    "      - training_logistic_loss: float\n",
    "      - training_accuracy: float\n",
    "      - training_AUC: float\n",
    "      - card_label: \"01\", \"02\", ...\n",
    "    \"\"\"\n",
    "    n_models = len(multipliers)\n",
    "    use_n = min(export_n if isinstance(export_n, int) else n_models, n_models)\n",
    "\n",
    "    payload = []\n",
    "    width = max(2, len(str(use_n)))  # zero-pad like \"01\"\n",
    "    for i in range(use_n):\n",
    "        mult = float(multipliers[i])\n",
    "        b0   = float(intercepts[i])\n",
    "        betas = np.asarray(coef_matrix[i], dtype=int)\n",
    "\n",
    "        # Use the native intercept for probability correctness.\n",
    "        clf = RiskScoreClassifier(mult, b0, betas)\n",
    "        clf.reset_featureNames(feature_names)\n",
    "\n",
    "        train_loss = float(clf.compute_logisticLoss(X_train, y_train))\n",
    "        train_acc, train_auc = clf.get_acc_and_auc(X_train, y_train)\n",
    "\n",
    "        payload.append({\n",
    "            \"feature_data\": _feature_pairs(betas, list(feature_names)),\n",
    "            \"risk_scale\": _risk_scale(mult, b0, betas),\n",
    "            \"training_logistic_loss\": train_loss,\n",
    "            \"training_accuracy\": float(train_acc),\n",
    "            \"training_AUC\": float(train_auc),\n",
    "            \"card_label\": f\"{i+1:0{width}d}\",\n",
    "        })\n",
    "\n",
    "    out_fname = os.path.join(OUTDIR, f\"{dataset_tag}.json\")\n",
    "    with open(out_fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[Riskomon] Wrote {out_fname} with {use_n} models (memo schema)\")\n",
    "    return out_fname\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Wire up your prepared splits & feature names\n",
    "# (These are defined in your dataset block.)\n",
    "# ---------------------------------------------\n",
    "SPLITS = {\n",
    "    \"STLMD\": (X_train_STLMD, y_train_STLMD, X_val_STLMD, y_val_STLMD, X_test_STLMD, y_test_STLMD, FEATURE_NAMES_STLMD),\n",
    "    \"STLM\":  (X_train_STLM,  y_train_STLM,  X_val_STLM,  y_val_STLM,  X_test_STLM,  y_test_STLM,  FEATURE_NAMES_STLM),\n",
    "    \"STPGLM\":(X_train_STPGLM,y_train_STPGLM,X_val_STPGLM,y_val_STPGLM,X_test_STPGLM,y_test_STPGLM,FEATURE_NAMES_STPGLM),\n",
    "}\n",
    "\n",
    "# ============================\n",
    "# Train / evaluate / export per design\n",
    "# ============================\n",
    "for name in DESIGNS_TO_RUN:\n",
    "    print(f\"\\n=== Processing: {name} ===\")\n",
    "    (X_train, y_train, X_val, y_val, X_test, y_test, feature_names) = SPLITS[name]\n",
    "\n",
    "    # build optimizer with Rashomon controls\n",
    "    opt = make_optimizer(\n",
    "        X=X_train, y=y_train,\n",
    "        k=SPARSITY_K, parent_size=PARENT_SIZE,\n",
    "        gap_tolerance=GAP_TOLERANCE,\n",
    "        select_top_m=SELECT_TOP_M,\n",
    "        max_attempts=MAX_ATTEMPTS,\n",
    "        want_intercept=True\n",
    "    )\n",
    "    print(f\"[opt] gap_tolerance={GAP_TOLERANCE}, select_top_m={SELECT_TOP_M}, maxAttempts={MAX_ATTEMPTS}, k={SPARSITY_K}, parent_size={PARENT_SIZE}\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    opt.optimize()\n",
    "    print(\"Optimization takes {:.2f} seconds.\".format(time.time() - t0))\n",
    "\n",
    "    multipliers, beta0_int, betas_int = extract_models(opt.get_models())\n",
    "    print(\"We generate {} risk score models from the sparse diverse pool\".format(len(multipliers)))\n",
    "\n",
    "    # metrics + probs (TRAIN / TEST)\n",
    "    train_aucs, train_accs, n_terms, train_probs = compute_model_metrics(\n",
    "        multipliers, beta0_int, betas_int, X_train, y_train\n",
    "    )\n",
    "    test_aucs,  test_accs,  _,        test_probs  = compute_model_metrics(\n",
    "        multipliers, beta0_int, betas_int, X_test,  y_test\n",
    "    )\n",
    "\n",
    "    # print features + AUCs per model\n",
    "    print_model_summaries(multipliers, beta0_int, betas_int, feature_names, train_aucs, test_aucs)\n",
    "\n",
    "    # save metrics CSV\n",
    "    card_labels = [f\"{i+1:02d}\" for i in range(len(multipliers))]\n",
    "    df_models = pd.DataFrame({\n",
    "        \"card\": card_labels,\n",
    "        \"n_terms\": n_terms,\n",
    "        \"train_auc\": train_aucs,\n",
    "        \"test_auc\":  test_aucs,\n",
    "        \"train_acc\": train_accs,\n",
    "        \"test_acc\":  test_accs,\n",
    "        \"intercept\": beta0_int,\n",
    "        \"multiplier\": multipliers,\n",
    "    })\n",
    "    csv_out = os.path.join(OUTDIR, f\"model_metrics_{name}.csv\")\n",
    "    df_models.sort_values(\"test_auc\", ascending=False).to_csv(csv_out, index=False)\n",
    "    print(f\"[AUC] Wrote {csv_out}\")\n",
    "\n",
    "    # ROC subplots for ALL models (TRAIN & TEST), 50 per page\n",
    "    y_train01 = ((y_train + 1.0) / 2.0).astype(int)\n",
    "    y_test01  = ((y_test  + 1.0) / 2.0).astype(int)\n",
    "\n",
    "    plot_roc_subplots_all(\n",
    "        y_true01=y_train01,\n",
    "        probs_list=train_probs,\n",
    "        labels=[f\"Card {c}\" for c in card_labels],\n",
    "        rows=SUBPLOT_ROWS, cols=SUBPLOT_COLS,\n",
    "        title_prefix=f\"{name} — TRAIN ROC\",\n",
    "        out_prefix=f\"roc_grid_{name}_train\"\n",
    "    )\n",
    "    plot_roc_subplots_all(\n",
    "        y_true01=y_test01,\n",
    "        probs_list=test_probs,\n",
    "        labels=[f\"Card {c}\" for c in card_labels],\n",
    "        rows=SUBPLOT_ROWS, cols=SUBPLOT_COLS,\n",
    "        title_prefix=f\"{name} — TEST ROC\",\n",
    "        out_prefix=f\"roc_grid_{name}_test\"\n",
    "    )\n",
    "\n",
    "    # ---- Riskomon JSON export (memo-compatible) ----\n",
    "    _ = export_riskomon_payload_memo(\n",
    "        multipliers=multipliers,\n",
    "        intercepts=beta0_int,\n",
    "        coef_matrix=betas_int,\n",
    "        feature_names=list(feature_names),\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        dataset_tag=f\"CANCER_{name}\",\n",
    "        export_n=None  # or an int like 50 if you want to cap\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57272266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335.99px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
