{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f3a676",
   "metadata": {},
   "source": [
    "# Data-Proceing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851e46b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:07:27.613461Z",
     "start_time": "2025-09-26T15:07:26.034125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filter] Removed 0 rows (invalid gender or Nodule_Type).\n",
      "[filter] Removed 0 rows (invalid gender or Nodule_Type).\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "Converting continuous features to binary features in the dataframe......\n",
      "We select thresholds for each continuous feature by sampling (without replacement) <= max_num_thresholds_per_feature values from all unique values in that feature column.\n",
      "Finish converting continuous features to binary features......\n",
      "STLMD   -> X_train (13324, 84), X_val (3282, 84), X_test (16425, 84)\n",
      "STLM    -> X_train (13324, 64), X_val (3282, 64), X_test (16425, 64)\n",
      "STPGLM  -> X_train (13324, 63), X_val (3282, 63), X_test (16425, 63)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fasterrisk.binarization_util import convert_continuous_df_to_binary_df\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "from fasterrisk.utils import download_file_from_google_drive\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "LABEL_COL = 'Cancer_lbl'\n",
    "FEATURES_STLMD  = ['sct_long_dia','part_solid','ground_glass','solid','Upper_Lobe','Spiculation','age','sex']\n",
    "FEATURES_STLM   = ['sct_long_dia','part_solid','ground_glass','solid','Upper_Lobe','Spiculation']\n",
    "FEATURES_STPGLM = ['sct_long_dia','part_solid','ground_glass','Upper_Lobe','Spiculation']\n",
    "\n",
    "CSV1 = '/data/usr/ft42/CVIT_XAI/LungRADS_Modeling/NLST_Statistics/ml_dataset/nlst_ct_nodule_df_set1.csv'\n",
    "CSV2 = '/data/usr/ft42/CVIT_XAI/LungRADS_Modeling/NLST_Statistics/ml_dataset/nlst_ct_nodule_df_set2.csv'\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def filter_and_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    initial_len = len(df)\n",
    "    df = df[df['gender'].isin(['male', 'female'])]\n",
    "    df = df[df['Nodule_Type'].isin(['solid', 'ground-glass', 'part-solid'])]\n",
    "    print(f\"[filter] Removed {initial_len - len(df)} rows (invalid gender or Nodule_Type).\")\n",
    "    return df\n",
    "\n",
    "def to_fastrisk_y(y_raw, pos_label=1) -> np.ndarray:\n",
    "    \"\"\"Return 1-D np.ndarray[float] with labels in {-1.0, +1.0}.\"\"\"\n",
    "    y_arr = np.asarray(y_raw).ravel()\n",
    "    uniq = set(np.unique(y_arr))\n",
    "    if uniq <= {0, 1}:\n",
    "        return (2 * y_arr - 1).astype(float)\n",
    "    return np.where(y_arr == pos_label, 1.0, -1.0).astype(float)\n",
    "\n",
    "def align_like_train(train_bin_df: pd.DataFrame, other_bin_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Make other_bin_df have exactly the same columns and order as train_bin_df.\"\"\"\n",
    "    cols = list(train_bin_df.columns)\n",
    "    return other_bin_df.reindex(columns=cols, fill_value=0)\n",
    "\n",
    "def prepare_data(df: pd.DataFrame, feature_cols, label_col):\n",
    "    X = df[feature_cols]\n",
    "    y = df[label_col]\n",
    "    return X, y\n",
    "\n",
    "def binarize_and_align(X_train_df: pd.DataFrame, X_val_df: pd.DataFrame, X_test_df: pd.DataFrame):\n",
    "    \"\"\"Binarize each split, then align val/test to training columns.\"\"\"\n",
    "    X_train_bin = convert_continuous_df_to_binary_df(X_train_df)\n",
    "    X_val_bin   = convert_continuous_df_to_binary_df(X_val_df)\n",
    "    X_test_bin  = convert_continuous_df_to_binary_df(X_test_df)\n",
    "    X_val_bin   = align_like_train(X_train_bin, X_val_bin)\n",
    "    X_test_bin  = align_like_train(X_train_bin, X_test_bin)\n",
    "    # sanity\n",
    "    assert list(X_val_bin.columns)  == list(X_train_bin.columns)\n",
    "    assert list(X_test_bin.columns) == list(X_train_bin.columns)\n",
    "    return X_train_bin, X_val_bin, X_test_bin\n",
    "\n",
    "# -------------------------\n",
    "# Load, filter, encode\n",
    "# -------------------------\n",
    "df1 = pd.read_csv(CSV1)\n",
    "df2 = pd.read_csv(CSV2)\n",
    "\n",
    "df1 = filter_and_report(df1)\n",
    "df2 = filter_and_report(df2)\n",
    "\n",
    "# Encode needed variables\n",
    "for df in (df1, df2):\n",
    "    df['sex'] = df['gender'].map({'male': 0, 'female': 1})\n",
    "    df['part_solid']   = df['Nodule_Type'].apply(lambda x: 1 if 'part-solid'   in x else 0)\n",
    "    df['ground_glass'] = df['Nodule_Type'].apply(lambda x: 1 if 'ground-glass' in x else 0)\n",
    "    df['solid']        = df['Nodule_Type'].apply(lambda x: 1 if 'solid'        in x else 0)\n",
    "\n",
    "# -------------------------\n",
    "# Patient-level stratified split (on df1)\n",
    "# -------------------------\n",
    "patients = df1[['pid', LABEL_COL]].drop_duplicates()\n",
    "train_patients, val_patients = train_test_split(\n",
    "    patients,\n",
    "    test_size=0.2,\n",
    "    stratify=patients[LABEL_COL],\n",
    "    random_state=42\n",
    ")\n",
    "train_df = df1[df1['pid'].isin(train_patients['pid'])]\n",
    "val_df   = df1[df1['pid'].isin(val_patients['pid'])]\n",
    "\n",
    "# ============================================================\n",
    "# STLMD (includes age & sex)\n",
    "# ============================================================\n",
    "X_train_STLMD_df, y_train_STLMD_raw = prepare_data(train_df, FEATURES_STLMD, LABEL_COL)\n",
    "X_val_STLMD_df,   y_val_STLMD_raw   = prepare_data(val_df,   FEATURES_STLMD, LABEL_COL)\n",
    "X_test_STLMD_df,  y_test_STLMD_raw  = prepare_data(df2,      FEATURES_STLMD, LABEL_COL)\n",
    "\n",
    "X_train_STLMD_bin, X_val_STLMD_bin, X_test_STLMD_bin = binarize_and_align(\n",
    "    X_train_STLMD_df, X_val_STLMD_df, X_test_STLMD_df\n",
    ")\n",
    "\n",
    "y_train_STLMD = to_fastrisk_y(y_train_STLMD_raw, pos_label=1)\n",
    "y_val_STLMD   = to_fastrisk_y(y_val_STLMD_raw,   pos_label=1)\n",
    "y_test_STLMD  = to_fastrisk_y(y_test_STLMD_raw,  pos_label=1)\n",
    "\n",
    "X_train_STLMD = X_train_STLMD_bin.to_numpy(dtype=float)\n",
    "X_val_STLMD   = X_val_STLMD_bin.to_numpy(dtype=float)\n",
    "X_test_STLMD  = X_test_STLMD_bin.to_numpy(dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# STLM (no age/sex)\n",
    "# ============================================================\n",
    "X_train_STLM_df, y_train_STLM_raw = prepare_data(train_df, FEATURES_STLM, LABEL_COL)\n",
    "X_val_STLM_df,   y_val_STLM_raw   = prepare_data(val_df,   FEATURES_STLM, LABEL_COL)\n",
    "X_test_STLM_df,  y_test_STLM_raw  = prepare_data(df2,      FEATURES_STLM, LABEL_COL)\n",
    "\n",
    "X_train_STLM_bin, X_val_STLM_bin, X_test_STLM_bin = binarize_and_align(\n",
    "    X_train_STLM_df, X_val_STLM_df, X_test_STLM_df\n",
    ")\n",
    "\n",
    "y_train_STLM = to_fastrisk_y(y_train_STLM_raw, pos_label=1)\n",
    "y_val_STLM   = to_fastrisk_y(y_val_STLM_raw,   pos_label=1)\n",
    "y_test_STLM  = to_fastrisk_y(y_test_STLM_raw,  pos_label=1)\n",
    "\n",
    "X_train_STLM = X_train_STLM_bin.to_numpy(dtype=float)\n",
    "X_val_STLM   = X_val_STLM_bin.to_numpy(dtype=float)\n",
    "X_test_STLM  = X_test_STLM_bin.to_numpy(dtype=float)\n",
    "\n",
    "# ============================================================\n",
    "# STPGLM (subset features)\n",
    "# ============================================================\n",
    "X_train_STPGLM_df, y_train_STPGLM_raw = prepare_data(train_df, FEATURES_STPGLM, LABEL_COL)\n",
    "X_val_STPGLM_df,   y_val_STPGLM_raw   = prepare_data(val_df,   FEATURES_STPGLM, LABEL_COL)\n",
    "X_test_STPGLM_df,  y_test_STPGLM_raw  = prepare_data(df2,      FEATURES_STPGLM, LABEL_COL)\n",
    "\n",
    "X_train_STPGLM_bin, X_val_STPGLM_bin, X_test_STPGLM_bin = binarize_and_align(\n",
    "    X_train_STPGLM_df, X_val_STPGLM_df, X_test_STPGLM_df\n",
    ")\n",
    "\n",
    "y_train_STPGLM = to_fastrisk_y(y_train_STPGLM_raw, pos_label=1)\n",
    "y_val_STPGLM   = to_fastrisk_y(y_val_STPGLM_raw,   pos_label=1)\n",
    "y_test_STPGLM  = to_fastrisk_y(y_test_STPGLM_raw,  pos_label=1)\n",
    "\n",
    "X_train_STPGLM = X_train_STPGLM_bin.to_numpy(dtype=float)\n",
    "X_val_STPGLM   = X_val_STPGLM_bin.to_numpy(dtype=float)\n",
    "X_test_STPGLM  = X_test_STPGLM_bin.to_numpy(dtype=float)\n",
    "\n",
    "# -------------------------\n",
    "# Quick hygiene checks\n",
    "# -------------------------\n",
    "def _chk(Xtr, ytr, Xv, yv, Xte, yte, name):\n",
    "    assert Xtr.shape[0] == ytr.shape[0] and Xv.shape[0] == yv.shape[0] and Xte.shape[0] == yte.shape[0], f\"row mismatch in {name}\"\n",
    "    assert set(np.unique(ytr)) <= {-1.0, 1.0} and set(np.unique(yv)) <= {-1.0, 1.0} and set(np.unique(yte)) <= {-1.0, 1.0}, f\"bad labels in {name}\"\n",
    "    print(f\"{name:7s} -> X_train {Xtr.shape}, X_val {Xv.shape}, X_test {Xte.shape}\")\n",
    "\n",
    "_chk(X_train_STLMD, y_train_STLMD, X_val_STLMD, y_val_STLMD, X_test_STLMD, y_test_STLMD, \"STLMD\")\n",
    "_chk(X_train_STLM,  y_train_STLM,  X_val_STLM,  y_val_STLM,  X_test_STLM,  y_test_STLM,  \"STLM\")\n",
    "_chk(X_train_STPGLM,y_train_STPGLM,X_val_STPGLM,y_val_STPGLM,X_test_STPGLM,y_test_STPGLM,\"STPGLM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b64b5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:07:27.631722Z",
     "start_time": "2025-09-26T15:07:27.615885Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Export FasterRisk models -> Riskomon JSON (memo-compatible)\n",
    "# ============================\n",
    "import json, math\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def _sigmoid(z: float) -> float:\n",
    "    return 1.0 / (1.0 + math.exp(-z))\n",
    "\n",
    "def _score_span(coefs: np.ndarray) -> (int, int):\n",
    "    \"\"\"Min/max integer score contribution from sparse integer weights (no intercept).\"\"\"\n",
    "    pos = int(coefs[coefs > 0].sum()) if coefs.size else 0\n",
    "    neg = int(coefs[coefs < 0].sum()) if coefs.size else 0\n",
    "    # min: include all negatives, exclude positives; max: include all positives\n",
    "    return neg, pos\n",
    "\n",
    "def _risk_scale(multiplier: float, intercept: int, coefs: np.ndarray) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Returns [[score_int, prob], ...] with probability = sigmoid(multiplier * (intercept + score)).\n",
    "    Matches the memo's 'risk_scale' field name.\n",
    "    \"\"\"\n",
    "    smin, smax = _score_span(coefs)\n",
    "    out = []\n",
    "    for s in range(smin, smax + 1):\n",
    "        total = intercept + s\n",
    "        p = _sigmoid(multiplier * total)\n",
    "        out.append([float(total), float(p)])   # memo JSON uses floats\n",
    "    return out\n",
    "\n",
    "def _feature_pairs(coefs: np.ndarray, feat_names: List[str]) -> List[List[Any]]:\n",
    "    \"\"\"\n",
    "    [[coef, \"FeatureName\"], ...] â€” coef first, only nonzeros, sorted by |coef| desc.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for w, name in zip(coefs.tolist(), feat_names):\n",
    "        if int(w) != 0:\n",
    "            pairs.append([float(int(w)), str(name)])  # store as float to mirror memo example\n",
    "    pairs.sort(key=lambda x: (-abs(x[0]), x[1]))\n",
    "    return pairs\n",
    "\n",
    "'''\n",
    "def export_riskomon_payload_memo(\n",
    "    multipliers: List[float],\n",
    "    intercepts: List[int],\n",
    "    coef_matrix: List[np.ndarray],\n",
    "    feature_names: List[str],\n",
    "    X_train: np.ndarray, y_train: np.ndarray,\n",
    "    dataset_tag: str = \"CANCER_STLMD\",\n",
    "    export_n: int = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build JSON using memo schema:\n",
    "      - feature_data: [[coef, \"name\"], ...]\n",
    "      - risk_scale:   [[score, prob], ...]\n",
    "      - training_logistic_loss: float\n",
    "      - training_accuracy: float\n",
    "      - training_AUC: float\n",
    "      - card_label: \"01\", \"02\", ...\n",
    "    \"\"\"\n",
    "    n_models = len(multipliers)\n",
    "    use_n = min(export_n if isinstance(export_n, int) else n_models, n_models)\n",
    "\n",
    "    payload = []\n",
    "    width = max(2, len(str(use_n)))  # zero-pad like \"01\"\n",
    "    for i in range(use_n):\n",
    "        mult = float(multipliers[i])\n",
    "        b0   = float(intercepts[i])\n",
    "        b0_shift = b0 + 1.0              # +1 point\n",
    "        betas = np.asarray(coef_matrix[i], dtype=int)\n",
    "\n",
    "        clf = RiskScoreClassifier(mult, b0_shift, betas)\n",
    "        clf.reset_featureNames(feature_names)\n",
    "\n",
    "        train_loss = float(clf.compute_logisticLoss(X_train, y_train))\n",
    "        train_acc, train_auc = clf.get_acc_and_auc(X_train, y_train)\n",
    "\n",
    "        payload.append({\n",
    "            \"feature_data\": _feature_pairs(betas, list(feature_names)),\n",
    "            \"risk_scale\": _risk_scale(mult, b0_shift, betas),\n",
    "            \"training_logistic_loss\": train_loss,\n",
    "            \"training_accuracy\": float(train_acc),\n",
    "            \"training_AUC\": float(train_auc),          # <-- capitalization per memo\n",
    "            \"card_label\": f\"{i+1:0{width}d}\",          # <-- \"01\", \"02\", ...\n",
    "        })\n",
    "\n",
    "    out_fname = f\"{dataset_tag}.json\"\n",
    "    with open(out_fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[Riskomon] Wrote {out_fname} with {len(payload)} models (memo schema)\")\n",
    "    return out_fname\n",
    "'''\n",
    "\n",
    "def export_riskomon_payload_memo(\n",
    "    multipliers: List[float],\n",
    "    intercepts: List[int],\n",
    "    coef_matrix: List[np.ndarray],\n",
    "    feature_names: List[str],\n",
    "    X_train: np.ndarray, y_train: np.ndarray,\n",
    "    dataset_tag: str = \"CANCER_STLMD\",\n",
    "    export_n: int = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build JSON using memo schema:\n",
    "      - feature_data: [[coef, \"name\"], ...]\n",
    "      - risk_scale:   [[score, prob], ...]\n",
    "      - training_logistic_loss: float\n",
    "      - training_accuracy: float\n",
    "      - training_AUC: float\n",
    "      - card_label: \"01\", \"02\", ...\n",
    "    \"\"\"\n",
    "    n_models = len(multipliers)\n",
    "    use_n = min(export_n if isinstance(export_n, int) else n_models, n_models)\n",
    "\n",
    "    payload = []\n",
    "    width = max(2, len(str(use_n)))  # zero-pad like \"01\"\n",
    "    for i in range(use_n):\n",
    "        mult = float(multipliers[i])\n",
    "        b0   = float(intercepts[i])\n",
    "        b0_shift = b0 + 1.0              # +1 point\n",
    "        betas = np.asarray(coef_matrix[i], dtype=int)\n",
    "        # compute points on a reference split (e.g., train)\n",
    "        points_train = (X_train @ betas)\n",
    "        C = -points_train.min()          # or -np.floor(points_train.min())\n",
    "        # new display score and intercept that preserve probabilities\n",
    "        b0_rebased = b0 - C              # undo the shift in the intercept\n",
    "\n",
    "        clf = RiskScoreClassifier(mult, b0_rebased, betas)\n",
    "        clf.reset_featureNames(feature_names)\n",
    "\n",
    "        train_loss = float(clf.compute_logisticLoss(X_train, y_train))\n",
    "        train_acc, train_auc = clf.get_acc_and_auc(X_train, y_train)\n",
    "\n",
    "        payload.append({\n",
    "            \"feature_data\": _feature_pairs(betas, list(feature_names)),\n",
    "            \"risk_scale\": _risk_scale(mult, b0_rebased, betas),\n",
    "            \"training_logistic_loss\": train_loss,\n",
    "            \"training_accuracy\": float(train_acc),\n",
    "            \"training_AUC\": float(train_auc),          # <-- capitalization per memo\n",
    "            \"card_label\": f\"{i+1:0{width}d}\",          # <-- \"01\", \"02\", ...\n",
    "        })\n",
    "\n",
    "    out_fname = f\"{dataset_tag}.json\"\n",
    "    with open(out_fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[Riskomon] Wrote {out_fname} with {len(payload)} models (memo schema)\")\n",
    "    return out_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61bd393a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T15:13:05.281043Z",
     "start_time": "2025-09-26T15:07:27.636601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:STLMD\n",
      "Optimization takes 336.77 seconds.\n",
      "We generate 50 risk score models from the sparse diverse pool\n",
      "[Riskomon] Wrote CANCER_STLMD.json with 50 models (memo schema)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for RiskInputs in [\"STLMD\"]:#\"STLM\",\"STPGLM\"]:\n",
    "    print('Processing:{}'.format(RiskInputs))\n",
    "\n",
    "    #-| Parameters\n",
    "    sparsity    = 5\n",
    "    parent_size = 10\n",
    "\n",
    "    if RiskInputs == \"STPGLM\":\n",
    "        X_train = X_train_STPGLM # X: ensure plain float np.array (not DataFrame)\n",
    "        y_train = y_train_STPGLM # y: Series -> 1D float np.array in {-1,+1}\n",
    "        X_test  = X_test_STPGLM # X: ensure plain float np.array (not DataFrame)\n",
    "        y_test  = y_test_STPGLM # y: Series -> 1D float np.array in {-1,+1}\n",
    "        X_train_bin = X_train_STPGLM_bin\n",
    "    elif RiskInputs == \"STLM\":\n",
    "        X_train = X_train_STLM # X: ensure plain float np.array (not DataFrame)\n",
    "        y_train = y_train_STLM # y: Series -> 1D float np.array in {-1,+1}\n",
    "        X_test  = X_test_STLM # X: ensure plain float np.array (not DataFrame)\n",
    "        y_test  = y_test_STLM # y: Series -> 1D float np.array in {-1,+1}\n",
    "        X_train_bin = X_train_STLM_bin\n",
    "    elif RiskInputs == \"STLMD\":\n",
    "        X_train = X_train_STLMD # X: ensure plain float np.array (not DataFrame)\n",
    "        y_train = y_train_STLMD # y: Series -> 1D float np.array in {-1,+1}\n",
    "        X_test  = X_test_STLMD # X: ensure plain float np.array (not DataFrame)\n",
    "        y_test  = y_test_STLMD # y: Series -> 1D float np.array in {-1,+1}\n",
    "        X_train_bin = X_train_STLMD_bin\n",
    "\n",
    "\n",
    "    #-- Optimizer\n",
    "    RiskScoreOptimizer_m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity, parent_size = parent_size)\n",
    "    start_time           = time.time()\n",
    "    RiskScoreOptimizer_m.optimize()\n",
    "    print(\"Optimization takes {:.2f} seconds.\".format(time.time() - start_time))\n",
    "    multipliers, sparseDiversePool_beta0_integer, sparseDiversePool_betas_integer = RiskScoreOptimizer_m.get_models()\n",
    "    print(\"We generate {} risk score models from the sparse diverse pool\".format(len(multipliers)))\n",
    "    # ===== Keep this; needed for feature names =====\n",
    "    X_featureNames = X_train_bin.columns\n",
    "\n",
    "    # ===== Paste the exporter block here (exactly here) =====\n",
    "    # [PASTE THE WHOLE EXPORTER I GAVE YOU]\n",
    "    _ = export_riskomon_payload_memo(\n",
    "        multipliers=multipliers,\n",
    "        intercepts=sparseDiversePool_beta0_integer,\n",
    "        coef_matrix=sparseDiversePool_betas_integer,\n",
    "        feature_names=list(X_featureNames),\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        dataset_tag=f\"CANCER_{RiskInputs}\",\n",
    "        export_n=None  # or 50\n",
    "    )\n",
    "    # ===== end paste ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b806f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
